{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEPJnSZCfRAJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split, validation_curve, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import silhouette_score \n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSuW_NoAfFCE"
   },
   "outputs": [],
   "source": [
    "# # Mount Collab to Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trN3wfns4AJS"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ls4UmA1wfVNg"
   },
   "outputs": [],
   "source": [
    "# file = '/content/drive/MyDrive/[3-NavSafe] INDENG 290 DATA-X/alternative_classification_data_localized.csv'\n",
    "file = 'alternative_classification_data_localized.csv'\n",
    "df_data = pd.read_csv(file, index_col=0)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-mPa5zr-QYp"
   },
   "outputs": [],
   "source": [
    "# neighborhood_file = '/content/drive/MyDrive/[3-NavSafe] INDENG 290 DATA-X/data_neighborhood_safety.csv'\n",
    "neighborhood_file = 'data_neighborhood_safety.csv'\n",
    "neighborhood = pd.read_csv(neighborhood_file)\n",
    "neighborhood.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQvQ7Kof8tfB"
   },
   "outputs": [],
   "source": [
    "df_all = df_data.merge(neighborhood, how='left', left_on='Analysis Neighborhood', right_on='Neighborhood').drop(['Analysis Neighborhood','Neighborhood'],axis=1)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmJCIAh8jwXE"
   },
   "outputs": [],
   "source": [
    "def safety_calc(row):\n",
    "    if row['Time Seg'] == 'Morning':\n",
    "        return row['Average of safe_day']\n",
    "    elif row['Time Seg'] == 'Afternoon':\n",
    "        return row['Average of safe_rate']\n",
    "    else:\n",
    "        return row['Average of safe_night']\n",
    "\n",
    "df_all['Safe'] = df_all.apply(lambda row: safety_calc(row), axis=1)\n",
    "df_all = df_all.drop(['Average of safe_day','Average of safe_night','Average of safe_rate'],axis=1)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2YQCLL53DkMq"
   },
   "outputs": [],
   "source": [
    "df_all['Avoid'] = 0\n",
    "# df_all.loc[(df_all['Average of safe_rate']<3.67) & (df_all['1.0']>10), 'Avoid'] = 1\n",
    "df_all.loc[(df_all['1.0']>75) | (df_all['2.0']>100) | (df_all['3.0']>200), 'Avoid'] = 1\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKQmiIc2a_GW"
   },
   "outputs": [],
   "source": [
    "time = pd.get_dummies(df_all['Time Seg'],drop_first=True)\n",
    "df_train = pd.concat([time, df_all.drop(['NewLat','NewLon','Time Seg'],axis=1)], axis=1)\n",
    "# df_train[['NewLat','NewLon','Evening','Morning','Night','1.0','2.0','3.0','4.0','5.0','6.0','Safe','Avoid']].head()\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1VMmqNCcHth"
   },
   "source": [
    "# Supervised - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R20nuN-t5FCw"
   },
   "source": [
    "This is the old modeling method, which has the quasi-complete seperation problem. We keep it here to show the learning path but it will be replaced by unsupervised methods later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lmXEdAAfVdj5"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df_train.drop('Avoid',axis=1), df_train['Avoid'], test_size=0.3, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2p0R_YUcGU2"
   },
   "outputs": [],
   "source": [
    "def plot_cv_curve(hyperparm_grid,train_scores,val_scores):\n",
    "    ax = plt.subplot(111)\n",
    "    ax.errorbar(hyperparm_grid,np.mean(train_scores,axis=1),yerr=np.std(train_scores,axis=1),label=\"train\")\n",
    "    ax.errorbar(hyperparm_grid,np.mean(val_scores,axis=1),yerr=np.std(val_scores,axis=1),label=\"validation\")\n",
    "    ax.set_xlabel('Hyperparameter')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pX2-jRRKW-Ha"
   },
   "outputs": [],
   "source": [
    "kf = KFold(5, shuffle=True, random_state=10)\n",
    "\n",
    "C_grid = np.logspace(-2,2,10)\n",
    "\n",
    "features = ['1.0','2.0','3.0','4.0','5.0','6.0']\n",
    "logit_pipe = Pipeline([('columns', ColumnTransformer([('keep', StandardScaler(with_mean=False), features)], \n",
    "                                                     remainder='passthrough')), \n",
    "                       ('logit', LogisticRegression(max_iter=5000, solver='newton-cg'))])\n",
    "train_scores, val_scores = validation_curve(logit_pipe, x_train, y_train, \n",
    "                                            param_name='logit__C', param_range=C_grid, cv=kf)\n",
    "\n",
    "ax = plot_cv_curve(C_grid,train_scores,val_scores)\n",
    "ax.set_xlabel('C')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7AsuHJyKSuGY"
   },
   "outputs": [],
   "source": [
    "logit_final = Pipeline([('columns', ColumnTransformer([('keep', StandardScaler(with_mean=False), features)], remainder='passthrough')), \n",
    "                       ('logit', LogisticRegression(max_iter=5000, solver='newton-cg', C=10))])\n",
    "logit_final.fit(x_train, y_train)\n",
    "pred = logit_final.predict_proba(x_test)[:,1]\n",
    "y_pred = [1 if i >=0.5 else 0 for i in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mpbt_hIeSt60"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_hHgfC9hILr"
   },
   "outputs": [],
   "source": [
    "print (\"\\nPrecision:\", tp/(tp+fp))\n",
    "print (\"\\nRecall:\", tp/(tp+fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82u8ZM3-K52C"
   },
   "source": [
    "# Unsupervised - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cd0optbL5pIU"
   },
   "source": [
    "We realize that this is an unsupervised machine learning problem. So in this session, we will explore different clustering methods and tune hyperparameters to train them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLu6fg_sMEP1"
   },
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/clustering-algorithms-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGcVGB00rFog"
   },
   "outputs": [],
   "source": [
    "df_cluster = df_train.drop(['Avoid'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTwNDAKOrO0c"
   },
   "outputs": [],
   "source": [
    "df_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfUzFKH1q9oL"
   },
   "source": [
    "## K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSSSldPDENLB"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1JSoZQ8xfmj"
   },
   "source": [
    "### Tuning\n",
    "\n",
    "In this sub-session, we will use elbow methods to tune the number of clusters with different metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FDiTe4G-rYr5"
   },
   "outputs": [],
   "source": [
    "distortions = []\n",
    "inertias = []\n",
    "mapping1 = {}\n",
    "mapping2 = {}\n",
    "K = range(1, 15)\n",
    " \n",
    "for k in K:\n",
    "    # Building and fitting the model\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(df_cluster)\n",
    "    kmeanModel.fit(df_cluster)\n",
    " \n",
    "    distortions.append(sum(np.min(cdist(df_cluster, kmeanModel.cluster_centers_,\n",
    "                                        'euclidean'), axis=1)) / df_cluster.shape[0])\n",
    "    inertias.append(kmeanModel.inertia_)\n",
    " \n",
    "    mapping1[k] = sum(np.min(cdist(df_cluster, kmeanModel.cluster_centers_,\n",
    "                                   'euclidean'), axis=1)) / df_cluster.shape[0]\n",
    "    mapping2[k] = kmeanModel.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n0sDa8Aisboo"
   },
   "outputs": [],
   "source": [
    "for key, val in mapping1.items():\n",
    "    print(f'{key} : {val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tE5n0-qcsfA1"
   },
   "outputs": [],
   "source": [
    "plt.plot(K, inertias, 'bx-')\n",
    "plt.xlabel('Values of K')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('The Elbow Method using Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UIqpvNawb2be"
   },
   "outputs": [],
   "source": [
    "# Silhouette Score for K means\n",
    "\n",
    "# Import ElbowVisualizer\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "model = KMeans(random_state =10)\n",
    "# k is range of number of clusters.\n",
    "visualizer = KElbowVisualizer(model, k=range(2,15,2),metric='silhouette', timings= True, locate_elbow=True)\n",
    "visualizer.fit(df_cluster)        # Fit the data to the visualizer\n",
    "plt.xlabel('Values of K')\n",
    "plt.ylabel('silhouette score')\n",
    "plt.title('The Elbow Method using silhouette')\n",
    "plt.show()\n",
    "# visualizer.show()        # Finalize and render the figure\n",
    "\n",
    "df3 = pd.DataFrame(visualizer.k_values_,columns=['centers'])\n",
    "df3['scores'] = visualizer.k_scores_\n",
    "df4 = df3[df3.scores == df3.scores.max()]\n",
    "print('Optimal number of clusters based on silhouette score:', df4['centers'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VV24ihH6cNEm"
   },
   "outputs": [],
   "source": [
    "# Calinski Harabasz Score for K means\n",
    "\n",
    "# Import ElbowVisualizer\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "model = KMeans(random_state =10)\n",
    "# k is range of number of clusters.\n",
    "visualizer = KElbowVisualizer(model, k=range(2,15,2),metric='calinski_harabaz', timings= True, locate_elbow=True)\n",
    "visualizer.fit(df_cluster)        # Fit the data to the visualizer\n",
    "plt.xlabel('Values of K')\n",
    "plt.ylabel('calinski harabasz score')\n",
    "plt.title('The Elbow Method using calinski harabasz')\n",
    "plt.show()\n",
    "# visualizer.show()        # Finalize and render the figure\n",
    "\n",
    "\n",
    "df3 = pd.DataFrame(visualizer.k_values_,columns=['centers'])\n",
    "df3['scores'] = visualizer.k_scores_\n",
    "df4 = df3[df3.scores == df3.scores.max()]\n",
    "print('Optimal number of clusters based on calinski harabasz:', df4['centers'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1rLkbGb7BE7"
   },
   "outputs": [],
   "source": [
    "# distortion Score for K means\n",
    "\n",
    "# Import ElbowVisualizer\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "model = KMeans(random_state =10)\n",
    "# k is range of number of clusters.\n",
    "visualizer = KElbowVisualizer(model, k=range(2,15,2),metric='distortion', timings= True, locate_elbow=True)\n",
    "visualizer.fit(df_cluster)        # Fit the data to the visualizer\n",
    "# visualizer.show()        # Finalize and render the figure\n",
    "\n",
    "\n",
    "df3 = pd.DataFrame(visualizer.k_values_,columns=['centers'])\n",
    "df3['scores'] = visualizer.k_scores_\n",
    "df4 = df3[df3.scores == df3.scores.max()]\n",
    "print('Optimal number of clusters based on distortion:', df4['centers'].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1dcXTq86-8D"
   },
   "source": [
    "Since different metrics give quite different k results, we choose k=4 and k=10 as the possible optimal k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wEAB5SPDxooq"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jkt2MubpttwF"
   },
   "outputs": [],
   "source": [
    "from numpy import unique\n",
    "from numpy import where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9jxWrUB7iC8"
   },
   "source": [
    "First, k=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2w5EELxdtYNz"
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "kmean_model = KMeans(n_clusters=4)\n",
    "# fit the model\n",
    "kmean_model.fit(df_cluster)\n",
    "# assign a cluster to each example\n",
    "kmean_yhat = kmean_model.predict(df_cluster)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(kmean_yhat)\n",
    "# # create scatter plot for samples from each cluster\n",
    "# for cluster in clusters:\n",
    "# \t# get row indexes for samples with this cluster\n",
    "# \trow_ix = where(yhat == cluster)\n",
    "# \t# create scatter of these samples\n",
    "# \tplt.scatter(df_kmean[row_ix, 0], df_kmean[row_ix, 1])\n",
    "# # show the plot\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnG3mgtsIaUj"
   },
   "outputs": [],
   "source": [
    "score_kmean_s = silhouette_score(df_cluster, kmean_model.labels_, metric='euclidean')\n",
    "score_kmean_c = calinski_harabasz_score(df_cluster, kmean_model.labels_)\n",
    "score_kmean_d = davies_bouldin_score(df_cluster, kmean_yhat)\n",
    "print('Silhouette Score: %.4f' % score_kmean_s)\n",
    "print('Calinski Harabasz Score: %.4f' % score_kmean_c)\n",
    "print('Davies Bouldin Score: %.4f' % score_kmean_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-87AzcRGAhj"
   },
   "outputs": [],
   "source": [
    "df_kmean = df_data.copy()\n",
    "df_kmean['Safe'] = df_cluster['Safe']\n",
    "df_kmean['Cluster'] = kmean_yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4BRoZSAuCpN"
   },
   "outputs": [],
   "source": [
    "pd.Series(kmean_yhat).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DG-t_fEquWEW"
   },
   "outputs": [],
   "source": [
    "# check what this cluster looks like\n",
    "df_kmean[df_kmean['Cluster']==2].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bABC4DWa7t1_"
   },
   "source": [
    "Then, k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NOOG5fO9uzv"
   },
   "outputs": [],
   "source": [
    "# old kmeans k=10\n",
    "kmean_model_10 = KMeans(n_clusters=10)\n",
    "# fit the model\n",
    "kmean_model_10.fit(df_cluster)\n",
    "# assign a cluster to each example\n",
    "kmean_yhat_10 = kmean_model_10.predict(df_cluster)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(kmean_yhat_10)\n",
    "df_kmean_10 = df_data.copy()\n",
    "df_kmean_10['Safe'] = df_cluster['Safe']\n",
    "df_kmean_10['Cluster'] = kmean_yhat_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jk7KPgxl95de"
   },
   "outputs": [],
   "source": [
    "score_kmean_s = silhouette_score(df_cluster, kmean_model_10.labels_, metric='euclidean')\n",
    "score_kmean_c = calinski_harabasz_score(df_cluster, kmean_model_10.labels_)\n",
    "score_kmean_d = davies_bouldin_score(df_cluster, kmean_yhat_10)\n",
    "print('Silhouette Score: %.4f' % score_kmean_s)\n",
    "print('Calinski Harabasz Score: %.4f' % score_kmean_c)\n",
    "print('Davies Bouldin Score: %.4f' % score_kmean_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pVDnTepIoZZ"
   },
   "source": [
    "The result is ery random, based on the initial points choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0L2QL7iWEEXv"
   },
   "source": [
    "## Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iADYe2byB89"
   },
   "source": [
    "### Tuning\n",
    "\n",
    "In this sub-session, we will use elbow methods to tune the number of clusters with different metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qboEX_E2LeaL"
   },
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch\n",
    "dendrogram = sch.dendrogram(sch.linkage(df_cluster, method  = \"ward\"))\n",
    "plt.title('Dendrogram')\n",
    "plt.ylabel('Euclidean distances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrheN7MmCryX"
   },
   "outputs": [],
   "source": [
    "# Silhouette Score for agg\n",
    "\n",
    "# Import ElbowVisualizer\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "model = AgglomerativeClustering()\n",
    "# k is range of number of clusters.\n",
    "visualizer = KElbowVisualizer(model, k=range(2,15,2),metric='silhouette', timings= True, locate_elbow=True)\n",
    "visualizer.fit(df_cluster)        # Fit the data to the visualizer\n",
    "plt.xlabel('Values of K')\n",
    "plt.ylabel('silhouette score')\n",
    "plt.title('The Elbow Method using silhouette')\n",
    "plt.show()\n",
    "# visualizer.show()        # Finalize and render the figure\n",
    "\n",
    "df3 = pd.DataFrame(visualizer.k_values_,columns=['centers'])\n",
    "df3['scores'] = visualizer.k_scores_\n",
    "df4 = df3[df3.scores == df3.scores.max()]\n",
    "print('Optimal number of clusters based on silhouette score:', df4['centers'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4zZXLuGDmT8"
   },
   "outputs": [],
   "source": [
    "# calinski_harabaz Score for agg\n",
    "\n",
    "# Import ElbowVisualizer\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "model = AgglomerativeClustering()\n",
    "# k is range of number of clusters.\n",
    "visualizer = KElbowVisualizer(model, k=range(2,15,2),metric='calinski_harabaz', timings= True, locate_elbow=True)\n",
    "visualizer.fit(df_cluster)        # Fit the data to the visualizer\n",
    "plt.xlabel('Values of K')\n",
    "plt.ylabel('calinski_harabaz score')\n",
    "plt.title('The Elbow Method using calinski_harabaz')\n",
    "plt.show()\n",
    "# visualizer.show()        # Finalize and render the figure\n",
    "\n",
    "df3 = pd.DataFrame(visualizer.k_values_,columns=['centers'])\n",
    "df3['scores'] = visualizer.k_scores_\n",
    "df4 = df3[df3.scores == df3.scores.max()]\n",
    "print('Optimal number of clusters based on calinski_harabaz score:', df4['centers'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQ6WSyqLD3Xj"
   },
   "outputs": [],
   "source": [
    "# distortion Score for agg\n",
    "\n",
    "# Import ElbowVisualizer\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "model = AgglomerativeClustering()\n",
    "# k is range of number of clusters.\n",
    "visualizer = KElbowVisualizer(model, k=range(2,15,2),metric='distortion', timings= True, locate_elbow=True)\n",
    "visualizer.fit(df_cluster)        # Fit the data to the visualizer\n",
    "plt.xlabel('Values of K')\n",
    "plt.ylabel('distortion score')\n",
    "plt.title('The Elbow Method using distortion')\n",
    "plt.show()\n",
    "# visualizer.show()        # Finalize and render the figure\n",
    "\n",
    "df3 = pd.DataFrame(visualizer.k_values_,columns=['centers'])\n",
    "df3['scores'] = visualizer.k_scores_\n",
    "df4 = df3[df3.scores == df3.scores.max()]\n",
    "print('Optimal number of clusters based on distortion score:', df4['centers'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRSxt3KuEZH2"
   },
   "source": [
    "Since different metrics give quite different k results, we choose k=3 and k=10 as the possible optimal k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgfFvzhYyEQq"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nps3LdZ0BA1b"
   },
   "source": [
    "agg, k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQc4Zx28C1eB"
   },
   "outputs": [],
   "source": [
    "# agglomerative clustering\n",
    "from numpy import unique\n",
    "from numpy import where\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define the model\n",
    "agg_model = AgglomerativeClustering(n_clusters=3)\n",
    "# fit model and predict clusters\n",
    "agg_yhat = agg_model.fit_predict(df_cluster)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(agg_yhat)\n",
    "# # create scatter plot for samples from each cluster\n",
    "# for cluster in clusters:\n",
    "# \t# get row indexes for samples with this cluster\n",
    "# \trow_ix = where(yhat == cluster)\n",
    "# \t# create scatter of these samples\n",
    "# \tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n",
    "# # show the plot\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3OdJZGHIqu5"
   },
   "outputs": [],
   "source": [
    "score_agg_s = silhouette_score(df_cluster, agg_model.labels_, metric='euclidean')\n",
    "score_agg_c = calinski_harabasz_score(df_cluster, agg_model.labels_)\n",
    "score_agg_d = davies_bouldin_score(df_cluster, agg_yhat)\n",
    "print('Silhouette Score: %.4f' % score_agg_s)\n",
    "print('Calinski Harabasz Score: %.4f' % score_agg_c)\n",
    "print('Davies Bouldin Score: %.4f' % score_agg_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tu5EypsBDSU0"
   },
   "outputs": [],
   "source": [
    "df_agg = df_data.copy()\n",
    "df_agg['Safe'] = df_cluster['Safe']\n",
    "df_agg['Cluster'] = agg_yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgJD0eTXE2n1"
   },
   "outputs": [],
   "source": [
    "df_agg['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FcE_amV1Hnvu"
   },
   "outputs": [],
   "source": [
    "df_agg[df_agg['Cluster']==1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGCE-qOzBVEi"
   },
   "source": [
    "agg, k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJMPmXSSBje4"
   },
   "outputs": [],
   "source": [
    "# agglomerative k=10\n",
    "agg_model_10 = AgglomerativeClustering(n_clusters=10)\n",
    "# fit the model\n",
    "# assign a cluster to each example\n",
    "agg_yhat_10 = agg_model_10.fit_predict(df_cluster)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(agg_yhat_10)\n",
    "df_agg_10 = df_data.copy()\n",
    "df_agg_10['Safe'] = df_cluster['Safe']\n",
    "df_agg_10['Cluster'] = agg_yhat_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWRPTuWpBwvg"
   },
   "outputs": [],
   "source": [
    "score_kmean_s = silhouette_score(df_cluster, agg_model_10.labels_, metric='euclidean')\n",
    "score_kmean_c = calinski_harabasz_score(df_cluster, agg_model_10.labels_)\n",
    "score_kmean_d = davies_bouldin_score(df_cluster, agg_yhat_10)\n",
    "print('Silhouette Score: %.4f' % score_kmean_s)\n",
    "print('Calinski Harabasz Score: %.4f' % score_kmean_c)\n",
    "print('Davies Bouldin Score: %.4f' % score_kmean_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MaN8wOKzJDMe"
   },
   "source": [
    "Similar algorithm as kmeans, but has a stable and fixed outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvilljsXIllc"
   },
   "source": [
    "## Gaussian mixture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQ9NBOhety_-"
   },
   "outputs": [],
   "source": [
    "# gaussian mixture clustering\n",
    "from numpy import unique\n",
    "from numpy import where\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3Vd6gjwyLZQ"
   },
   "source": [
    "### Tuning\n",
    "\n",
    "In this sub-session, we will use elbow methods to tune the number of clusters with different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGFD5qTSK1rB"
   },
   "outputs": [],
   "source": [
    "\n",
    "n_components = np.arange(2, 20, 2)\n",
    "models = [GaussianMixture(n_components=n).fit(df_cluster)\n",
    "          for n in n_components]\n",
    "\n",
    "plt.plot(n_components, [m.bic(df_cluster) for m in models], label='BIC')\n",
    "plt.plot(n_components, [m.aic(df_cluster) for m in models], label='AIC')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('n_components');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qcxMVn1aQqOq"
   },
   "outputs": [],
   "source": [
    "n_components = np.arange(2,20)\n",
    "models = [GaussianMixture(n_components=n).fit(df_cluster)\n",
    "          for n in n_components]\n",
    "m_yhat = [m.predict(df_cluster) for m in models]\n",
    "\n",
    "silhouette = []\n",
    "calinski = []\n",
    "davies = []\n",
    "for i in m_yhat:\n",
    "  silhouette.append(silhouette_score(df_cluster, i, metric='euclidean'))\n",
    "  # calinski.append(calinski_harabasz_score(df_cluster, i))\n",
    "  # davies.append(davies_bouldin_score(df_cluster, i))\n",
    "\n",
    "\n",
    "plt.plot(n_components, silhouette, label='silhouette')\n",
    "# plt.plot(n_components, calinski, label='calinski')\n",
    "# plt.plot(n_components, davies, label='davies')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('n_components');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6s_gEeAIUfY3"
   },
   "outputs": [],
   "source": [
    "n_components = range(2, 20,2)\n",
    "covariance_type = ['spherical', 'tied', 'diag', 'full']\n",
    "score=[]\n",
    "for cov in covariance_type:\n",
    "    for n_comp in n_components:\n",
    "        gmm = GaussianMixture(n_components=n_comp,covariance_type=cov, random_state = 10, max_iter=10000)\n",
    "        gmm.fit(df_cluster)\n",
    "        score.append((cov,n_comp,gmm.bic(df_cluster)))\n",
    "score_1 = pd.DataFrame(score)\n",
    "score_1.columns = ['Covariance_Type', 'N_Components','BIC_Score']\n",
    "score_2 = score_1[score_1.BIC_Score == score_1.BIC_Score.min()]\n",
    "\n",
    "score_2.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "794F_SD-VHeW"
   },
   "outputs": [],
   "source": [
    "# Silhouette Score for GMM\n",
    "\n",
    "\n",
    "n_components = range(2, 20,2)\n",
    "covariance_type = ['spherical', 'tied', 'diag', 'full']\n",
    "score=[]\n",
    "for cov in covariance_type:\n",
    "    for n_comp in n_components:\n",
    "        gmm=GaussianMixture(n_components=n_comp,covariance_type=cov,random_state = 10,max_iter=10000)\n",
    "        model = gmm.fit(df_cluster)\n",
    "        model_2 = model.predict(df_cluster)\n",
    "        score_s = silhouette_score(df_cluster, model_2, metric='euclidean')\n",
    "        score.append((cov,n_comp,score_s))\n",
    "score_1 = pd.DataFrame(score)\n",
    "score_1.columns = ['Covariance_Type', 'N_Components','Silhouette_Score']\n",
    "score_2 = score_1[score_1.Silhouette_Score == score_1.Silhouette_Score.max()]\n",
    "score_2.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySVZn82zWS-s"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calinski Harabasz Score for GMM\n",
    "\n",
    "\n",
    "n_components = range(2, 20,2)\n",
    "covariance_type = ['spherical', 'tied', 'diag', 'full']\n",
    "score=[]\n",
    "for cov in covariance_type:\n",
    "    for n_comp in n_components:\n",
    "        gmm=GaussianMixture(n_components=n_comp,covariance_type=cov, random_state = 10,max_iter=10000)\n",
    "        model = gmm.fit(df_cluster)\n",
    "        model_2 = model.predict(df_cluster)\n",
    "        score_c = calinski_harabasz_score(df_cluster, model_2)\n",
    "        score.append((cov,n_comp,score_c))\n",
    "score_1 = pd.DataFrame(score)\n",
    "score_1.columns = ['Covariance_Type', 'N_Components','Calinski_Harabasz_Score']\n",
    "score_2 = score_1[score_1.Calinski_Harabasz_Score == score_1.Calinski_Harabasz_Score.max()]\n",
    "score_2.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3u8y_zuuyN1B"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sBHrZtjFFEo"
   },
   "source": [
    "gmm, k=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vM63qk7IIqyc"
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "gmm_model = GaussianMixture(n_components=4, covariance_type='tied')\n",
    "# fit the model\n",
    "gmm_model.fit(df_cluster)\n",
    "# assign a cluster to each example\n",
    "gmm_yhat = gmm_model.predict(df_cluster)\n",
    "# # retrieve unique clusters\n",
    "# clusters = unique(yhat)\n",
    "# # create scatter plot for samples from each cluster\n",
    "# for cluster in clusters:\n",
    "# \t# get row indexes for samples with this cluster\n",
    "# \trow_ix = where(yhat == cluster)\n",
    "# \t# create scatter of these samples\n",
    "# \tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n",
    "# # show the plot\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lx3ah_lYI2BK"
   },
   "outputs": [],
   "source": [
    "score_gmm_s = silhouette_score(df_cluster, gmm_yhat, metric='euclidean')\n",
    "score_gmm_c = calinski_harabasz_score(df_cluster, gmm_yhat)\n",
    "score_gmm_d = davies_bouldin_score(df_cluster, gmm_yhat)\n",
    "print('Silhouette Score: %.4f' % score_gmm_s)\n",
    "print('Calinski Harabasz Score: %.4f' % score_gmm_c)\n",
    "print('Davies Bouldin Score: %.4f' % score_gmm_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "poA204bjJX4Z"
   },
   "outputs": [],
   "source": [
    "df_gmm = df_data.copy()\n",
    "df_gmm['Safe'] = df_cluster['Safe']\n",
    "df_gmm['Cluster'] = gmm_yhat\n",
    "df_gmm['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQrzha9QMN0m"
   },
   "outputs": [],
   "source": [
    "df_gmm[df_gmm['Cluster'].isin([1])].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NALPfXJOFV-v"
   },
   "source": [
    "gmm, k=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwUWZCfFFjki"
   },
   "outputs": [],
   "source": [
    "# gausian n_components = 16, cov_type = full\n",
    "gmm_model_16 = GaussianMixture(n_components=16, covariance_type='full')\n",
    "# fit the model\n",
    "gmm_model_16.fit(df_cluster)\n",
    "# assign a cluster to each example\n",
    "gmm_yhat_16= gmm_model_16.predict(df_cluster)\n",
    "df_gmm_16 = df_data.copy()\n",
    "df_gmm_16['Safe'] = df_cluster['Safe']\n",
    "df_gmm_16['Cluster'] = gmm_yhat_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6yMacNzFq11"
   },
   "outputs": [],
   "source": [
    "score_kmean_s = silhouette_score(df_cluster, gmm_yhat_16, metric='euclidean')\n",
    "score_kmean_c = calinski_harabasz_score(df_cluster, gmm_yhat_16)\n",
    "score_kmean_d = davies_bouldin_score(df_cluster, gmm_yhat_16)\n",
    "print('Silhouette Score: %.4f' % score_kmean_s)\n",
    "print('Calinski Harabasz Score: %.4f' % score_kmean_c)\n",
    "print('Davies Bouldin Score: %.4f' % score_kmean_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tjc5owRiRV4"
   },
   "source": [
    "# Map visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUSQ06lludK2"
   },
   "outputs": [],
   "source": [
    "!pip install gmaps\n",
    "!pip install ipywidgets\n",
    "!pip install widgetsnbextension\n",
    "import gmaps \n",
    "import ipywidgets as widgets\n",
    "from ipywidgets.embed import embed_minimal_html\n",
    "import IPython\n",
    "\n",
    "gmaps.configure(api_key='AIzaSyDgJrLjmtTKlpLjwAfmseJJ-w8ZEy_YHeM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohLjsGMy_tfZ"
   },
   "source": [
    "Assign weights [0.4, 0.3, 0.2, 0.1] to group0-group3 for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1LbValJ1qt-"
   },
   "outputs": [],
   "source": [
    "# rules = [0.4, 0.3, 0.2, 0.1] for group0 to group3\n",
    "def assign_weights(df, rules):\n",
    "  k = unique(df['Cluster'])\n",
    "  df['Weight'] = -1\n",
    "  for i in k:\n",
    "    weight = 0\n",
    "    for j in range(len(rules)):\n",
    "      weight += rules[j]*df.loc[df['Cluster']==i, str(float(j+1))].mean()\n",
    "\n",
    "    df.loc[df['Cluster']==i, 'Weight'] = weight\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qpVYEY3Fsz_"
   },
   "source": [
    "## kmeans, k=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mq3SW6NO8GOW"
   },
   "outputs": [],
   "source": [
    "df_1 = assign_weights(df_kmean, [0.4, 0.3, 0.2, 0.1])\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NKMwV3H78QfJ"
   },
   "outputs": [],
   "source": [
    "df_1['Weight'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lzxk_SJyiP6z"
   },
   "outputs": [],
   "source": [
    "# centers = df_1[(df_1['Cluster']==6) | (df_1['Cluster']==3) | (df_1['Cluster']==4) | (df_1['Cluster']==1) | (df_1['Cluster']==9) |(df_1['Cluster']==7)][['NewLat','NewLon','Time Seg']].drop_duplicates()\n",
    "centers = df_1[(df_1['Cluster'].isin([2,1,3,0])) & (df_1['Time Seg'].isin(['Evening']))][['NewLat','NewLon','Time Seg']].drop_duplicates()\n",
    "# [6,3,4,1,9,7]\n",
    "centers['Weight'] = df_1['Weight']\n",
    "# centers = df_1[['NewLat','NewLon']].drop_duplicates()\n",
    "# centers['Weight'] = df_1['Weight']\n",
    "\n",
    "locations = centers[['NewLat', 'NewLon']]\n",
    "weights = centers['Weight']\n",
    "fig = gmaps.figure() \n",
    "heatmap_layer = gmaps.heatmap_layer(locations, weights=weights) \n",
    "fig.add_layer(gmaps.heatmap_layer(locations, weights=weights)) \n",
    "\n",
    "embed_minimal_html('export.html', views=[fig])\n",
    "IPython.display.HTML(filename=\"export.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkhZU8H2ANfQ"
   },
   "source": [
    "## kmeans, k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vCU87CJfq2b"
   },
   "outputs": [],
   "source": [
    "df_1_10 = assign_weights(df_kmean_10, [0.4, 0.3, 0.2, 0.1])\n",
    "df_1_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P70yAQ2nfzUH"
   },
   "outputs": [],
   "source": [
    "df_1_10['Weight'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7JEnwxQBf3bR"
   },
   "outputs": [],
   "source": [
    "# centers = df_1[(df_1['Cluster']==6) | (df_1['Cluster']==3) | (df_1['Cluster']==4) | (df_1['Cluster']==1) | (df_1['Cluster']==9) |(df_1['Cluster']==7)][['NewLat','NewLon','Time Seg']].drop_duplicates()\n",
    "centers = df_1_10[(df_1_10['Cluster'].isin([9,5,4,6,1,3,8,2,7,0])) & (df_1_10['Time Seg'].isin(['Evening']))][['NewLat','NewLon','Time Seg']].drop_duplicates()\n",
    "# [6,3,4,1,9,7]\n",
    "centers['Weight'] = df_1_10['Weight']\n",
    "# centers = df_1[['NewLat','NewLon']].drop_duplicates()\n",
    "# centers['Weight'] = df_1['Weight']\n",
    "\n",
    "locations = centers[['NewLat', 'NewLon']]\n",
    "weights = centers['Weight']\n",
    "fig = gmaps.figure() \n",
    "heatmap_layer = gmaps.heatmap_layer(locations, weights=weights) \n",
    "fig.add_layer(gmaps.heatmap_layer(locations, weights=weights)) \n",
    "\n",
    "embed_minimal_html('export.html', views=[fig])\n",
    "IPython.display.HTML(filename=\"export.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9mfsJ-VFvsR"
   },
   "source": [
    "## agglomerative, k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7el01L6FyYo"
   },
   "outputs": [],
   "source": [
    "df_2 = assign_weights(df_agg, [0.4, 0.3, 0.2, 0.1])\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_l0zTO9Go9s"
   },
   "outputs": [],
   "source": [
    "df_2['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nX8tTI_nGwrs"
   },
   "outputs": [],
   "source": [
    "# centers = df_1[(df_1['Cluster']==6) | (df_1['Cluster']==3) | (df_1['Cluster']==4) | (df_1['Cluster']==1) | (df_1['Cluster']==9) |(df_1['Cluster']==7)][['NewLat','NewLon','Time Seg']].drop_duplicates()\n",
    "centers = df_2[(df_2['Cluster'].isin([0,2,1])) & (df_2['Time Seg']=='Evening')][['NewLat','NewLon','Time Seg']].drop_duplicates()\n",
    "# centers = df_2[(df_2['Cluster'].isin([1,2])) & (df_2['Time Seg']=='Evening')][['NewLat','NewLon','Time Seg']].drop_duplicates()\n",
    "\n",
    "# [6,3,2,1,8,7]\n",
    "centers['Weight'] = df_2['Weight']\n",
    "# centers = df_1[['NewLat','NewLon']].drop_duplicates()\n",
    "# centers['Weight'] = df_1['Weight']\n",
    "\n",
    "locations = centers[['NewLat', 'NewLon']]\n",
    "weights = centers['Weight']\n",
    "fig = gmaps.figure() \n",
    "heatmap_layer = gmaps.heatmap_layer(locations, weights=weights) \n",
    "fig.add_layer(gmaps.heatmap_layer(locations, weights=weights)) \n",
    "\n",
    "embed_minimal_html('export.html', views=[fig])\n",
    "IPython.display.HTML(filename=\"export.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAxqYMmdAUqq"
   },
   "source": [
    "## agglomerative, k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBo1zH3jnrWC"
   },
   "outputs": [],
   "source": [
    "df_2_10 = assign_weights(df_agg_10, [0.4, 0.3, 0.2, 0.1])\n",
    "df_2_10['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59wZnQdkg5PA"
   },
   "outputs": [],
   "source": [
    "score_kmean_s = silhouette_score(df_cluster, agg_model_10.labels_, metric='euclidean')\n",
    "score_kmean_c = calinski_harabasz_score(df_cluster, agg_model_10.labels_)\n",
    "score_kmean_d = davies_bouldin_score(df_cluster, agg_yhat_10)\n",
    "print('Silhouette Score: %.4f' % score_kmean_s)\n",
    "print('Calinski Harabasz Score: %.4f' % score_kmean_c)\n",
    "print('Davies Bouldin Score: %.4f' % score_kmean_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptKBvrtDowLS"
   },
   "outputs": [],
   "source": [
    "# centers = df_1[(df_1['Cluster']==6) | (df_1['Cluster']==3) | (df_1['Cluster']==4) | (df_1['Cluster']==1) | (df_1['Cluster']==9) |(df_1['Cluster']==7)][['NewLat','NewLon','Time Seg']].drop_duplicates()\n",
    "# centers = df_2_10[(df_2_10['Cluster'].isin([1,7,9,2,0,8,4])) & (df_2_10['Time Seg'].isin(['Evening']))][['NewLat','NewLon','Time Seg']].drop_duplicates()\n",
    "centers = df_2_10[(df_2_10['Time Seg'].isin(['Evening']))][['NewLat','NewLon','Time Seg']].drop_duplicates()\n",
    "# [6,3,4,1,9,7]\n",
    "centers['Weight'] = df_2_10['Weight']\n",
    "# centers = df_1[['NewLat','NewLon']].drop_duplicates()\n",
    "# centers['Weight'] = df_1['Weight']\n",
    "\n",
    "locations = centers[['NewLat', 'NewLon']]\n",
    "weights = centers['Weight']\n",
    "fig = gmaps.figure() \n",
    "heatmap_layer = gmaps.heatmap_layer(locations, weights=weights) \n",
    "fig.add_layer(gmaps.heatmap_layer(locations, weights=weights)) \n",
    "\n",
    "embed_minimal_html('export.html', views=[fig])\n",
    "IPython.display.HTML(filename=\"export.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CoRIy1Z2J5Zy"
   },
   "source": [
    "## gaussian, k=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oyLSCVLSJ8SY"
   },
   "outputs": [],
   "source": [
    "df_3 = assign_weights(df_gmm, [0.4, 0.3, 0.2, 0.1])\n",
    "df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blUJu6DxJ_rr"
   },
   "outputs": [],
   "source": [
    "df_3['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enjz4iQjKEKh"
   },
   "outputs": [],
   "source": [
    "# centers = df_1[(df_1['Cluster']==6) | (df_1['Cluster']==3) | (df_1['Cluster']==4) | (df_1['Cluster']==1) | (df_1['Cluster']==9) |(df_1['Cluster']==7)][['NewLat','NewLon','Time Seg']].drop_duplicates()\n",
    "# centers = df_3[(df_3['Cluster'].isin([1,3])) & (df_3['Time Seg']=='Evening')][['NewLat','NewLon','Time Seg']].drop_duplicates()\n",
    "centers = df_3[(df_3['Time Seg']=='Evening')][['NewLat','NewLon','Time Seg']].drop_duplicates()\n",
    "# centers = df_3[['NewLat','NewLon','Time Seg']].drop_duplicates()\n",
    "# [5,4,2,6,7,1]\n",
    "centers['Weight'] = df_3['Weight']\n",
    "# centers = df_1[['NewLat','NewLon']].drop_duplicates()\n",
    "# centers['Weight'] = df_1['Weight']\n",
    "\n",
    "locations = centers[['NewLat', 'NewLon']]\n",
    "weights = centers['Weight']\n",
    "fig = gmaps.figure() \n",
    "heatmap_layer = gmaps.heatmap_layer(locations, weights=weights) \n",
    "fig.add_layer(gmaps.heatmap_layer(locations, weights=weights)) \n",
    "\n",
    "embed_minimal_html('export.html', views=[fig])\n",
    "IPython.display.HTML(filename=\"export.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ue59h6muAf1N"
   },
   "source": [
    "## gaussian, k=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BIqRjX21rg0-"
   },
   "outputs": [],
   "source": [
    "df_3_16 = assign_weights(df_gmm_16, [0.4, 0.3, 0.2, 0.1])\n",
    "df_3_16['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vzWOsmcsk2o"
   },
   "outputs": [],
   "source": [
    "# centers = df_1[(df_1['Cluster']==6) | (df_1['Cluster']==3) | (df_1['Cluster']==4) | (df_1['Cluster']==1) | (df_1['Cluster']==9) |(df_1['Cluster']==7)][['NewLat','NewLon','Time Seg']].drop_duplicates()\n",
    "# centers = df_3_16[(df_3_16['Cluster'].isin([12,13,4,7,15,8,11,2,9,10,3,1])) & (df_3_16['Time Seg'].isin(['Evening']))][['NewLat','NewLon','Time Seg']].drop_duplicates()\n",
    "centers = df_3_16[(df_3_16['Time Seg'].isin(['Evening']))][['NewLat','NewLon','Time Seg']].drop_duplicates()\n",
    "# [6,3,4,1,9,7]\n",
    "centers['Weight'] = df_3_16['Weight']\n",
    "# centers = df_1[['NewLat','NewLon']].drop_duplicates()\n",
    "# centers['Weight'] = df_1['Weight']\n",
    "\n",
    "locations = centers[['NewLat', 'NewLon']]\n",
    "weights = centers['Weight']\n",
    "fig = gmaps.figure() \n",
    "heatmap_layer = gmaps.heatmap_layer(locations, weights=weights) \n",
    "fig.add_layer(gmaps.heatmap_layer(locations, weights=weights)) \n",
    "\n",
    "embed_minimal_html('export.html', views=[fig])\n",
    "IPython.display.HTML(filename=\"export.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yd7HTSPCFGP"
   },
   "source": [
    "## average weights\n",
    "\n",
    "for each unit, use the average of weights from all models to visualize the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3JV3z3lx3IW"
   },
   "outputs": [],
   "source": [
    "df_avg = df_1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IcTTqEQGCM8z"
   },
   "outputs": [],
   "source": [
    "df_avg.drop(columns=['Cluster'], inplace=True)\n",
    "df_avg['Weight'] = (df_1['Weight']+df_1_10['Weight']+df_2['Weight']+df_2_10['Weight']+df_3['Weight']+df_3_16['Weight'])/6\n",
    "df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qhotg-ndDKT2"
   },
   "outputs": [],
   "source": [
    "# centers = df_1[(df_1['Cluster']==6) | (df_1['Cluster']==3) | (df_1['Cluster']==4) | (df_1['Cluster']==1) | (df_1['Cluster']==9) |(df_1['Cluster']==7)][['NewLat','NewLon','Time Seg']].drop_duplicates()\n",
    "# centers = df_3_16[(df_3_16['Cluster'].isin([12,13,4,7,15,8,11,2,9,10,3,1])) & (df_3_16['Time Seg'].isin(['Evening']))][['NewLat','NewLon','Time Seg']].drop_duplicates()\n",
    "centers = df_avg[(df_avg['Time Seg'].isin(['Evening']))][['NewLat','NewLon','Time Seg']].drop_duplicates()\n",
    "# [6,3,4,1,9,7]\n",
    "centers['Weight'] = df_avg['Weight']\n",
    "# centers = df_1[['NewLat','NewLon']].drop_duplicates()\n",
    "# centers['Weight'] = df_1['Weight']\n",
    "\n",
    "locations = centers[['NewLat', 'NewLon']]\n",
    "weights = centers['Weight']\n",
    "fig = gmaps.figure() \n",
    "heatmap_layer = gmaps.heatmap_layer(locations, weights=weights) \n",
    "fig.add_layer(gmaps.heatmap_layer(locations, weights=weights)) \n",
    "\n",
    "embed_minimal_html('export.html', views=[fig])\n",
    "IPython.display.HTML(filename=\"export.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6AFycQCMx7c"
   },
   "source": [
    "# Compare w test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGbTpQRvHnfF"
   },
   "source": [
    "We create a test set with 120+ records to test the previous 7 models. We will classify the test set with each model result and generate scores for each model to evaluate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJuqPz7lYoan"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from itertools import compress, product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mOKEZSHmO19W"
   },
   "outputs": [],
   "source": [
    "# test_file = '/content/drive/MyDrive/[3-NavSafe] INDENG 290 DATA-X/data_testset.csv'\n",
    "test_file = 'data_testset.csv'\n",
    "df_test = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5qpDrAD7PCMK"
   },
   "outputs": [],
   "source": [
    "df_test.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-irm0x9P0oq"
   },
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8Acx2GYbAa5"
   },
   "outputs": [],
   "source": [
    "# print model clusters & prediction\n",
    "def compare_model1(df, df_test):\n",
    "  df_cluster_info = df['Cluster'].value_counts()\n",
    "  print(df_cluster_info)\n",
    "\n",
    "  df_comp = df_test.merge(df, on=['NewLat','NewLon','Time Seg'], how='left')\n",
    "\n",
    "  print(df_comp[['Avoid','Cluster']].value_counts())\n",
    "  return df_comp, df_cluster_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XzSNIMSpbk7c"
   },
   "outputs": [],
   "source": [
    "# print accuracy / recall / precision\n",
    "def compare_model2(df_comp, rules = [2]):\n",
    "  df_comp['pred'] = df_comp['Cluster'].apply(lambda x: 0 if x in rules else 1)\n",
    "\n",
    "  tn, fp, fn, tp = confusion_matrix(df_comp['Avoid'], df_comp['pred']).ravel()\n",
    "  accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "  recall = (tp)/(fn+tp)\n",
    "  precision = (tp)/(fp+tp)\n",
    "  # print('accuracy: {}'.format(accuracy))\n",
    "  # print('recall: {}'.format((recall))\n",
    "  # print('precision: {}'.format(precision))\n",
    "  return accuracy, recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqqnAoxWdrz8"
   },
   "outputs": [],
   "source": [
    "# define the rules for classifying: combinations of all clusters except the last one\n",
    "def find_rules(df_cluster_info, end=-1):\n",
    "  avoid_0 = [df_cluster_info[0]]\n",
    "  items = df_cluster_info[1:end]\n",
    "  comb = list(( set(compress(items,mask)) for mask in product(*[[0,1]]*len(items)) ))\n",
    "  return [avoid_0+list(i) for i in comb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ERbkhuxhHoN"
   },
   "outputs": [],
   "source": [
    "# find the rule with highest recall\n",
    "def check_rules(df_comp, rules):\n",
    "  accuracy = []\n",
    "  recall = []\n",
    "  precision = []\n",
    "  for i in rules:\n",
    "    a, b, c = compare_model2(df_comp, i)\n",
    "    accuracy.append(a)\n",
    "    recall.append(b)\n",
    "    precision.append(c)\n",
    "  table = pd.DataFrame({'rule':rules, 'accuracy':accuracy, 'recall':recall, 'precision':precision})\n",
    "  return table[table['recall']==table['recall'].max()], table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xdHsLBfsumm"
   },
   "outputs": [],
   "source": [
    "# %load_ext google.colab.data_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bgptn70sZ9HE"
   },
   "source": [
    "## kmeans, k=4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X9oAxA9zb46m"
   },
   "outputs": [],
   "source": [
    "df_1_comp, df_1_cluster_info = compare_model1(df_1, df_test)\n",
    "\n",
    "df_1_rules = find_rules(df_1_cluster_info.index.tolist())\n",
    "print(df_1_rules)\n",
    "\n",
    "df_1_result, df_1_table = check_rules(df_1_comp, df_1_rules)\n",
    "print(df_1_result)\n",
    "display(df_1_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRfQaky7oUK3"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(4), df_1_table['accuracy'], label='accuracy')\n",
    "plt.plot(range(4), df_1_table['recall'], label='recall')\n",
    "plt.plot(range(4), df_1_table['precision'], label='precision')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('rules');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89xv6g-3kMzA"
   },
   "source": [
    "## kmeans, k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0XunwqjkFty"
   },
   "outputs": [],
   "source": [
    "df_1_10_comp, df_1_10_cluster_info = compare_model1(df_1_10, df_test)\n",
    "\n",
    "df_1_10_rules = find_rules(df_1_10_cluster_info.index.tolist())\n",
    "print(df_1_10_rules)\n",
    "\n",
    "df_1_10_result, df_1_10_table = check_rules(df_1_10_comp, df_1_10_rules)\n",
    "print(df_1_10_result)\n",
    "display(df_1_10_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cjB9O_T_kGDh"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(50,20))\n",
    "plt.plot(range(256), df_1_10_table['accuracy'], label='accuracy')\n",
    "plt.plot(range(256), df_1_10_table['recall'], label='recall')\n",
    "plt.plot(range(256), df_1_10_table['precision'], label='precision')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('rules');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nK6CNuvlkGZg"
   },
   "outputs": [],
   "source": [
    "df_1_10_table.loc[192]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1HIi57nt7KJ"
   },
   "source": [
    "## agglomerative, k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BIptEZWDkGuP"
   },
   "outputs": [],
   "source": [
    "df_2_comp, df_2_cluster_info = compare_model1(df_2, df_test)\n",
    "\n",
    "df_2_rules = find_rules(df_2_cluster_info.index.tolist())\n",
    "print(df_2_rules)\n",
    "\n",
    "df_2_result, df_2_table = check_rules(df_2_comp, df_2_rules)\n",
    "print(df_2_result)\n",
    "display(df_2_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihgJyc8uurSL"
   },
   "source": [
    "## agglomerative, k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8qJOmRJkHHt"
   },
   "outputs": [],
   "source": [
    "df_2_10_comp, df_2_10_cluster_info = compare_model1(df_2_10, df_test)\n",
    "\n",
    "df_2_10_rules = find_rules(df_2_10_cluster_info.index.tolist())\n",
    "print(df_2_10_rules)\n",
    "\n",
    "df_2_10_result, df_2_10_table = check_rules(df_2_10_comp, df_2_10_rules)\n",
    "print(df_2_10_result)\n",
    "display(df_2_10_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kU1ajz7uql4"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(50,20))\n",
    "plt.plot(range(df_2_10_table.shape[0]), df_2_10_table['accuracy'], label='accuracy')\n",
    "plt.plot(range(df_2_10_table.shape[0]), df_2_10_table['recall'], label='recall')\n",
    "plt.plot(range(df_2_10_table.shape[0]), df_2_10_table['precision'], label='precision')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('rules');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQcXmEiIkHd1"
   },
   "outputs": [],
   "source": [
    "print(df_2_10_table.loc[128])\n",
    "print(df_2_10_table.loc[192])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kP9mF4rMw9sB"
   },
   "source": [
    "## gaussian, k=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22lhn3YzkHxA"
   },
   "outputs": [],
   "source": [
    "df_3_comp, df_3_cluster_info = compare_model1(df_3, df_test)\n",
    "\n",
    "df_3_rules = find_rules(df_3_cluster_info.index.tolist())\n",
    "print(df_3_rules)\n",
    "\n",
    "df_3_result, df_3_table = check_rules(df_3_comp, df_3_rules)\n",
    "print(df_3_result)\n",
    "display(df_3_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYrYNfhixM9b"
   },
   "source": [
    "## gaussian, k=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s0aSsK7pkIE4"
   },
   "outputs": [],
   "source": [
    "df_3_16_comp, df_3_16_cluster_info = compare_model1(df_3_16, df_test)\n",
    "\n",
    "df_3_16_rules = find_rules(df_3_16_cluster_info.index.tolist())\n",
    "print(df_3_16_rules)\n",
    "\n",
    "df_3_16_result, df_3_16_table = check_rules(df_3_16_comp, df_3_16_rules)\n",
    "print(df_3_16_result)\n",
    "display(df_3_16_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oIAOUK52xLFs"
   },
   "outputs": [],
   "source": [
    "print(df_3_16_table.loc[12288])\n",
    "print(df_3_16_table.loc[14336])\n",
    "print(df_3_16_table.loc[15360])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ha2w8GHLxLrv"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(50,20))\n",
    "plt.plot(range(df_3_16_table.shape[0]), df_3_16_table['accuracy'], label='accuracy')\n",
    "plt.plot(range(df_3_16_table.shape[0]), df_3_16_table['recall'], label='recall')\n",
    "plt.plot(range(df_3_16_table.shape[0]), df_3_16_table['precision'], label='precision')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('rules');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vavwL0f1zWFa"
   },
   "source": [
    "## average model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NOAB1ni1FHr"
   },
   "outputs": [],
   "source": [
    "df_avg['Cluster'] = pd.cut(df_avg['Weight'], 30, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeblxfVPxMYB"
   },
   "outputs": [],
   "source": [
    "df_avg_comp, df_avg_cluster_info = compare_model1(df_avg, df_test)\n",
    "\n",
    "df_avg_rules = find_rules(df_avg_cluster_info.index.tolist(), 9)\n",
    "print(df_avg_rules)\n",
    "\n",
    "df_avg_result, df_avg_table = check_rules(df_avg_comp, df_avg_rules)\n",
    "print(df_avg_result)\n",
    "display(df_avg_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RwHDA5xZzkYt"
   },
   "outputs": [],
   "source": [
    "print(df_avg_table.loc[128])\n",
    "print(df_avg_table.loc[192])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhEvtGmnA0nF"
   },
   "source": [
    "## compare result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsh5d_ZYA4tG"
   },
   "outputs": [],
   "source": [
    "# %unload_ext google.colab.data_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5QPWJsTJW_c"
   },
   "outputs": [],
   "source": [
    "df_all_compare = pd.DataFrame({'k,k=4':df_1_table.loc[0],\n",
    "                               'k,k=10':df_1_10_table.loc[192],\n",
    "                               'agg,k=3':df_2_table.loc[0],\n",
    "                               'agg,k=10':df_2_10_table.loc[128],\n",
    "                               'gmm,k=4':df_3_table.loc[0],\n",
    "                               'gmm,k=16':df_3_16_table.loc[12288],\n",
    "                               'avg':df_avg_table.loc[128]})\n",
    "df_all_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xPoPRiTC_2s"
   },
   "source": [
    "agg,10 performs better than k,4;\n",
    "\n",
    "gmm,4 performs better than avg;\n",
    "\n",
    "agg,3 performs similarly to k,10, but more stable;\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "accuracy: gmm,16 -> gmm,4 -> agg,3 ->agg,10\n",
    "\n",
    "recall: agg,10 -> gmm,4 -> agg,3 / gmm,16\n",
    "\n",
    "precision: gmm,16 -> gmm,4 -> agg,3 -> agg,10\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**If we want best recall, agg,10**\n",
    "\n",
    "**If we want a balance overall, gmm,4 / gmm,16**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-6C_An6YFv7"
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TsU06bHNYEKo"
   },
   "outputs": [],
   "source": [
    "def predict_avoid(df, safe_cluster_num):\n",
    "  cluster = df['Cluster'].value_counts().index.tolist()\n",
    "  safe = cluster[:safe_cluster_num]\n",
    "  df['pred'] = df['Cluster'].apply(lambda x: 0 if x in safe else 1)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWRYUDGLZJ7Q"
   },
   "outputs": [],
   "source": [
    "df_1_pred = predict_avoid(df_1, 1)\n",
    "df_1_10_pred = predict_avoid(df_1_10, 3)\n",
    "df_2_pred = predict_avoid(df_2, 1)\n",
    "df_2_10_pred = predict_avoid(df_2_10, 2)\n",
    "df_3_pred = predict_avoid(df_3, 1)\n",
    "df_3_16_pred = predict_avoid(df_3_16, 3)\n",
    "df_avg_pred = predict_avoid(df_avg, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uGe9u8EEXjq0"
   },
   "outputs": [],
   "source": [
    "# df_2_10_pred.to_csv('final_prediction_agg_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WYqVFG7_afKR"
   },
   "outputs": [],
   "source": [
    "df_1_10_pred['pred'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EpaDziOFqCe"
   },
   "source": [
    "# Compare by routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KykN4vfOKcJ_"
   },
   "source": [
    "The following session will use each model to test on the sample routes. Each subsession will generate the required input (avoid areas) for HERE API to provide the safe route result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMW6xiqMMLsA"
   },
   "outputs": [],
   "source": [
    "def find_active_area(start_lat, start_lon, end_lat, end_lon, val):\n",
    "  if abs(start_lat) > abs(end_lat):\n",
    "    act_start_lat = (abs(start_lat)+val)*np.sign(start_lat)\n",
    "    act_end_lat = (abs(end_lat)-val)*np.sign(end_lat)\n",
    "  else:\n",
    "    act_start_lat = (abs(start_lat)-val)*np.sign(start_lat)\n",
    "    act_end_lat = (abs(end_lat)+val)*np.sign(end_lat)\n",
    "\n",
    "  if abs(start_lon) > abs(end_lon):\n",
    "    act_start_lon = (abs(start_lon)+val)*np.sign(start_lon)\n",
    "    act_end_lon = (abs(end_lon)-val)*np.sign(end_lon)\n",
    "  else:\n",
    "    act_start_lon = (abs(start_lon)-val)*np.sign(start_lon)\n",
    "    act_end_lon = (abs(end_lon)+val)*np.sign(end_lon)\n",
    "\n",
    "    return act_start_lat, act_start_lon, act_end_lat, act_end_lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QxLkljK-UUuG"
   },
   "outputs": [],
   "source": [
    "def find_related_cluster(act_start_lat, act_start_lon, act_end_lat, act_end_lon, time_seg, df):\n",
    "  df_time = df[df['Time Seg']==time_seg]\n",
    "\n",
    "  if act_start_lat>act_end_lat:\n",
    "    lat_range = [act_end_lat, act_start_lat]\n",
    "  else:\n",
    "    lat_range = [act_start_lat, act_end_lat]\n",
    "\n",
    "  if act_start_lon>act_end_lon:\n",
    "    lon_range = [act_end_lon, act_start_lat]\n",
    "  else:\n",
    "    lon_range = [act_start_lat, act_end_lat]\n",
    "\n",
    "  df_area = df_time[(df_time['NewLat']>=lat_range[0]) & (df_time['NewLat']<=lat_range[1]) & \n",
    "                    (df_time['NewLon']>=lon_range[0]) & (df_time['NewLon']<=lon_range[1])]\n",
    "  return df_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqnJ4VvsFzS5"
   },
   "source": [
    "**Caltrain to Brenda's Soul Food**\t\n",
    "start: 37.7766711\t-122.3970318\t\n",
    "end: 37.781409\t-122.4178537\t\n",
    "time: 7:00 PM - afternoon\n",
    "\n",
    "**16th St BART to Dolores Park**\t\n",
    "start:37.7646383\t-122.4201503  \n",
    "end:37.761652\t-122.423218\t\n",
    "time:10:00 PM - evening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmbRshd5dI0m"
   },
   "source": [
    "## test route 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BHtHLh9MRSI"
   },
   "outputs": [],
   "source": [
    "act_start_lat, act_start_lon, act_end_lat, act_end_lon = find_active_area(37.7766711,-122.3970318,37.781409,-122.4178537, 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6X1XKLqaWqeI"
   },
   "source": [
    "### kmean, k=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FfWw6_9gVyWc"
   },
   "outputs": [],
   "source": [
    "df_related_cluster_1 = find_related_cluster(act_start_lat, act_start_lon, act_end_lat, act_end_lon, 'Afternoon', df_1_pred)\n",
    "df_related_cluster_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_d28lj9QXf7y"
   },
   "outputs": [],
   "source": [
    "centers = df_related_cluster_1[['NewLat','NewLon']].drop_duplicates()\n",
    "# [6,3,4,1,9,7]\n",
    "centers['Weight'] = df_related_cluster_1['Weight']*df_related_cluster_1['pred']\n",
    "# centers = df_1[['NewLat','NewLon']].drop_duplicates()\n",
    "# centers['Weight'] = df_1['Weight']\n",
    "\n",
    "locations = centers[['NewLat', 'NewLon']]\n",
    "weights = centers['Weight']\n",
    "fig = gmaps.figure() \n",
    "heatmap_layer = gmaps.heatmap_layer(locations, weights=weights) \n",
    "fig.add_layer(gmaps.heatmap_layer(locations, weights=weights)) \n",
    "\n",
    "embed_minimal_html('export.html', views=[fig])\n",
    "IPython.display.HTML(filename=\"export.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1pLcGbeZcJn9"
   },
   "outputs": [],
   "source": [
    "avoid_area = centers.sort_values(by=['Weight'], ascending=False).head(10)\n",
    "avoid_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_gC0NLKBcuMT"
   },
   "outputs": [],
   "source": [
    "avoid_list = avoid_area[['NewLat','NewLon']].apply(lambda x: 'bbox:' + str(x['NewLon']-0.00125) + ',' + str(x['NewLat']-0.00125) + \n",
    "                                                   ',' + str(x['NewLon']+0.00125) + ',' + str(x['NewLat']+0.00125), axis=1).values\n",
    "avoid_param = '|'.join(avoid_list)\n",
    "avoid_rec = []\n",
    "for i in range(len(avoid_list)):\n",
    "  area_label = avoid_list[i][5:].split(',')\n",
    "  avoid_rec.append((float(area_label[3]), float(area_label[0]), float(area_label[1]), float(area_label[2])))\n",
    "\n",
    "avoid_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ix0XsKdPLw2Y"
   },
   "outputs": [],
   "source": [
    "avoid_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5D5d4FSKXXOR"
   },
   "source": [
    "### kmeans, k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rfyVIu-kXcvO"
   },
   "outputs": [],
   "source": [
    "df_related_cluster_1_10 = find_related_cluster(act_start_lat, act_start_lon, act_end_lat, act_end_lon, 'Afternoon', df_1_10_pred)\n",
    "df_related_cluster_1_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSuDPI0RYH2g"
   },
   "outputs": [],
   "source": [
    "centers = df_related_cluster_1_10[['NewLat','NewLon']].drop_duplicates()\n",
    "# [6,3,4,1,9,7]\n",
    "centers['Weight'] = df_related_cluster_1_10['Weight']*df_related_cluster_1_10['pred']\n",
    "# centers = df_1[['NewLat','NewLon']].drop_duplicates()\n",
    "# centers['Weight'] = df_1['Weight']\n",
    "\n",
    "locations = centers[['NewLat', 'NewLon']]\n",
    "weights = centers['Weight']\n",
    "fig = gmaps.figure() \n",
    "heatmap_layer = gmaps.heatmap_layer(locations, weights=weights) \n",
    "fig.add_layer(gmaps.heatmap_layer(locations, weights=weights)) \n",
    "\n",
    "embed_minimal_html('export.html', views=[fig])\n",
    "IPython.display.HTML(filename=\"export.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4pIjNi6YITM"
   },
   "outputs": [],
   "source": [
    "avoid_area = centers.sort_values(by=['Weight'], ascending=False).head(10)\n",
    "avoid_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usm-rLrxYIZu"
   },
   "outputs": [],
   "source": [
    "avoid_list = avoid_area[['NewLat','NewLon']].apply(lambda x: 'bbox:' + str(x['NewLon']-0.00125) + ',' + str(x['NewLat']-0.00125) + \n",
    "                                                   ',' + str(x['NewLon']+0.00125) + ',' + str(x['NewLat']+0.00125), axis=1).values\n",
    "avoid_param = '|'.join(avoid_list)\n",
    "avoid_rec = []\n",
    "for i in range(len(avoid_list)):\n",
    "  area_label = avoid_list[i][5:].split(',')\n",
    "  avoid_rec.append((float(area_label[3]), float(area_label[0]), float(area_label[1]), float(area_label[2])))\n",
    "\n",
    "avoid_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5IzirOw3YI4R"
   },
   "outputs": [],
   "source": [
    "avoid_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HneW6MEaM_I"
   },
   "source": [
    "### agg, k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TbLGiJUjYJqU"
   },
   "outputs": [],
   "source": [
    "df_related_cluster_2 = find_related_cluster(act_start_lat, act_start_lon, act_end_lat, act_end_lon, 'Afternoon', df_2_pred)\n",
    "df_related_cluster_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYvreSTvYJwk"
   },
   "outputs": [],
   "source": [
    "centers = df_related_cluster_2[['NewLat','NewLon']].drop_duplicates()\n",
    "# [6,3,4,1,9,7]\n",
    "centers['Weight'] = df_related_cluster_2['Weight']*df_related_cluster_2['pred']\n",
    "# centers = df_1[['NewLat','NewLon']].drop_duplicates()\n",
    "# centers['Weight'] = df_1['Weight']\n",
    "\n",
    "locations = centers[['NewLat', 'NewLon']]\n",
    "weights = centers['Weight']\n",
    "fig = gmaps.figure() \n",
    "heatmap_layer = gmaps.heatmap_layer(locations, weights=weights) \n",
    "fig.add_layer(gmaps.heatmap_layer(locations, weights=weights)) \n",
    "\n",
    "embed_minimal_html('export.html', views=[fig])\n",
    "IPython.display.HTML(filename=\"export.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fPlENgBJaXZB"
   },
   "outputs": [],
   "source": [
    "avoid_area = centers.sort_values(by=['Weight'], ascending=False).head(10)\n",
    "avoid_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cqsy9rCqYKMq"
   },
   "outputs": [],
   "source": [
    "avoid_list = avoid_area[['NewLat','NewLon']].apply(lambda x: 'bbox:' + str(x['NewLon']-0.00125) + ',' + str(x['NewLat']-0.00125) + \n",
    "                                                   ',' + str(x['NewLon']+0.00125) + ',' + str(x['NewLat']+0.00125), axis=1).values\n",
    "avoid_param = '|'.join(avoid_list)\n",
    "avoid_rec = []\n",
    "for i in range(len(avoid_list)):\n",
    "  area_label = avoid_list[i][5:].split(',')\n",
    "  avoid_rec.append((float(area_label[3]), float(area_label[0]), float(area_label[1]), float(area_label[2])))\n",
    "\n",
    "avoid_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vwvfz-unYK8l"
   },
   "outputs": [],
   "source": [
    "avoid_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEtnTqgpbuc8"
   },
   "source": [
    "### agg, k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4K6pDSNqYLBo"
   },
   "outputs": [],
   "source": [
    "df_related_cluster_2_10 = find_related_cluster(act_start_lat, act_start_lon, act_end_lat, act_end_lon, 'Afternoon', df_2_10_pred)\n",
    "df_related_cluster_2_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RToIguIhYLcV"
   },
   "outputs": [],
   "source": [
    "centers = df_related_cluster_2_10[['NewLat','NewLon']].drop_duplicates()\n",
    "# [6,3,4,1,9,7]\n",
    "centers['Weight'] = df_related_cluster_2_10['Weight']*df_related_cluster_2_10['pred']\n",
    "# centers = df_1[['NewLat','NewLon']].drop_duplicates()\n",
    "# centers['Weight'] = df_1['Weight']\n",
    "\n",
    "locations = centers[['NewLat', 'NewLon']]\n",
    "weights = centers['Weight']\n",
    "fig = gmaps.figure() \n",
    "heatmap_layer = gmaps.heatmap_layer(locations, weights=weights) \n",
    "fig.add_layer(gmaps.heatmap_layer(locations, weights=weights)) \n",
    "\n",
    "embed_minimal_html('export.html', views=[fig])\n",
    "IPython.display.HTML(filename=\"export.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ulNpezh1YL5W"
   },
   "outputs": [],
   "source": [
    "avoid_area = centers.sort_values(by=['Weight'], ascending=False).head(10)\n",
    "avoid_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DoWh5vAsYMW6"
   },
   "outputs": [],
   "source": [
    "avoid_list = avoid_area[['NewLat','NewLon']].apply(lambda x: 'bbox:' + str(x['NewLon']-0.00125) + ',' + str(x['NewLat']-0.00125) + \n",
    "                                                   ',' + str(x['NewLon']+0.00125) + ',' + str(x['NewLat']+0.00125), axis=1).values\n",
    "avoid_param = '|'.join(avoid_list)\n",
    "avoid_rec = []\n",
    "for i in range(len(avoid_list)):\n",
    "  area_label = avoid_list[i][5:].split(',')\n",
    "  avoid_rec.append((float(area_label[3]), float(area_label[0]), float(area_label[1]), float(area_label[2])))\n",
    "\n",
    "avoid_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGJ_OLc8NRts"
   },
   "outputs": [],
   "source": [
    "avoid_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dG7baFQ_dE-3"
   },
   "source": [
    "### gmm, k=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "McWuf1l0cNfz"
   },
   "outputs": [],
   "source": [
    "df_related_cluster_3 = find_related_cluster(act_start_lat, act_start_lon, act_end_lat, act_end_lon, 'Afternoon', df_3_pred)\n",
    "df_related_cluster_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ju1dHWPc_G8"
   },
   "outputs": [],
   "source": [
    "centers = df_related_cluster_3[['NewLat','NewLon']].drop_duplicates()\n",
    "# [6,3,4,1,9,7]\n",
    "centers['Weight'] = df_related_cluster_3['Weight']*df_related_cluster_3['pred']\n",
    "# centers = df_1[['NewLat','NewLon']].drop_duplicates()\n",
    "# centers['Weight'] = df_1['Weight']\n",
    "\n",
    "locations = centers[['NewLat', 'NewLon']]\n",
    "weights = centers['Weight']\n",
    "fig = gmaps.figure() \n",
    "heatmap_layer = gmaps.heatmap_layer(locations, weights=weights) \n",
    "fig.add_layer(gmaps.heatmap_layer(locations, weights=weights)) \n",
    "\n",
    "embed_minimal_html('export.html', views=[fig])\n",
    "IPython.display.HTML(filename=\"export.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmPIpAAadBLU"
   },
   "outputs": [],
   "source": [
    "avoid_area = centers.sort_values(by=['Weight'], ascending=False).head(10)\n",
    "avoid_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNFQloUFdBbf"
   },
   "outputs": [],
   "source": [
    "avoid_list = avoid_area[['NewLat','NewLon']].apply(lambda x: 'bbox:' + str(x['NewLon']-0.00125) + ',' + str(x['NewLat']-0.00125) + \n",
    "                                                   ',' + str(x['NewLon']+0.00125) + ',' + str(x['NewLat']+0.00125), axis=1).values\n",
    "avoid_param = '|'.join(avoid_list)\n",
    "avoid_rec = []\n",
    "for i in range(len(avoid_list)):\n",
    "  area_label = avoid_list[i][5:].split(',')\n",
    "  avoid_rec.append((float(area_label[3]), float(area_label[0]), float(area_label[1]), float(area_label[2])))\n",
    "\n",
    "avoid_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBhFe1gUdByV"
   },
   "outputs": [],
   "source": [
    "avoid_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaZyjycLeVnP"
   },
   "source": [
    "### gmm,k =16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Of0RAjfPdCCG"
   },
   "outputs": [],
   "source": [
    "df_related_cluster_3_16 = find_related_cluster(act_start_lat, act_start_lon, act_end_lat, act_end_lon, 'Afternoon', df_3_16_pred)\n",
    "df_related_cluster_3_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wr-6eASSdCPW"
   },
   "outputs": [],
   "source": [
    "centers = df_related_cluster_3_16[['NewLat','NewLon']].drop_duplicates()\n",
    "# [6,3,4,1,9,7]\n",
    "centers['Weight'] = df_related_cluster_3_16['Weight']*df_related_cluster_3_16['pred']\n",
    "# centers = df_1[['NewLat','NewLon']].drop_duplicates()\n",
    "# centers['Weight'] = df_1['Weight']\n",
    "\n",
    "locations = centers[['NewLat', 'NewLon']]\n",
    "weights = centers['Weight']\n",
    "fig = gmaps.figure() \n",
    "heatmap_layer = gmaps.heatmap_layer(locations, weights=weights) \n",
    "fig.add_layer(gmaps.heatmap_layer(locations, weights=weights)) \n",
    "\n",
    "embed_minimal_html('export.html', views=[fig])\n",
    "IPython.display.HTML(filename=\"export.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJ5xAMsOdCdq"
   },
   "outputs": [],
   "source": [
    "avoid_area = centers.sort_values(by=['Weight'], ascending=False).head(10)\n",
    "avoid_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLld1dy6dCuV"
   },
   "outputs": [],
   "source": [
    "avoid_list = avoid_area[['NewLat','NewLon']].apply(lambda x: 'bbox:' + str(x['NewLon']-0.00125) + ',' + str(x['NewLat']-0.00125) + \n",
    "                                                   ',' + str(x['NewLon']+0.00125) + ',' + str(x['NewLat']+0.00125), axis=1).values\n",
    "avoid_param = '|'.join(avoid_list)\n",
    "avoid_rec = []\n",
    "for i in range(len(avoid_list)):\n",
    "  area_label = avoid_list[i][5:].split(',')\n",
    "  avoid_rec.append((float(area_label[3]), float(area_label[0]), float(area_label[1]), float(area_label[2])))\n",
    "\n",
    "avoid_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ame2ixs3dDE9"
   },
   "outputs": [],
   "source": [
    "avoid_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SE4AuN-Sfsei"
   },
   "source": [
    "### avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbBAtKP2dDTM"
   },
   "outputs": [],
   "source": [
    "df_related_cluster_avg = find_related_cluster(act_start_lat, act_start_lon, act_end_lat, act_end_lon, 'Afternoon', df_avg_pred)\n",
    "df_related_cluster_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sinqtK1SdDhx"
   },
   "outputs": [],
   "source": [
    "centers = df_related_cluster_avg[['NewLat','NewLon']].drop_duplicates()\n",
    "# [6,3,4,1,9,7]\n",
    "centers['Weight'] = df_related_cluster_avg['Weight']*df_related_cluster_avg['pred']\n",
    "# centers = df_1[['NewLat','NewLon']].drop_duplicates()\n",
    "# centers['Weight'] = df_1['Weight']\n",
    "\n",
    "locations = centers[['NewLat', 'NewLon']]\n",
    "weights = centers['Weight']\n",
    "fig = gmaps.figure() \n",
    "heatmap_layer = gmaps.heatmap_layer(locations, weights=weights) \n",
    "fig.add_layer(gmaps.heatmap_layer(locations, weights=weights)) \n",
    "\n",
    "embed_minimal_html('export.html', views=[fig])\n",
    "IPython.display.HTML(filename=\"export.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YhMT6Z-VdDxr"
   },
   "outputs": [],
   "source": [
    "avoid_area = centers.sort_values(by=['Weight'], ascending=False).head(10)\n",
    "avoid_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5J0Jb_LdEBg"
   },
   "outputs": [],
   "source": [
    "avoid_list = avoid_area[['NewLat','NewLon']].apply(lambda x: 'bbox:' + str(x['NewLon']-0.00125) + ',' + str(x['NewLat']-0.00125) + \n",
    "                                                   ',' + str(x['NewLon']+0.00125) + ',' + str(x['NewLat']+0.00125), axis=1).values\n",
    "avoid_param = '|'.join(avoid_list)\n",
    "avoid_rec = []\n",
    "for i in range(len(avoid_list)):\n",
    "  area_label = avoid_list[i][5:].split(',')\n",
    "  avoid_rec.append((float(area_label[3]), float(area_label[0]), float(area_label[1]), float(area_label[2])))\n",
    "\n",
    "avoid_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4gXSNOR7f9Cp"
   },
   "outputs": [],
   "source": [
    "avoid_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlgfJL2Ujfjp"
   },
   "source": [
    "## test route 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msnfKBbVjfjq"
   },
   "outputs": [],
   "source": [
    "act_start_lat, act_start_lon, act_end_lat, act_end_lon = find_active_area(37.7646383,-122.4201503,37.761652,-122.423218, 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jl2M1u5Cjfjr"
   },
   "source": [
    "### kmean, k=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SltJ9_Rgjfjs"
   },
   "outputs": [],
   "source": [
    "df_related_cluster_1 = find_related_cluster(act_start_lat, act_start_lon, act_end_lat, act_end_lon, 'Evening', df_1_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNO21fG7jfjs"
   },
   "outputs": [],
   "source": [
    "df_related_cluster_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wk5adixUjfjt"
   },
   "outputs": [],
   "source": [
    "centers = df_related_cluster_1[['NewLat','NewLon']].drop_duplicates()\n",
    "# [6,3,4,1,9,7]\n",
    "centers['Weight'] = df_related_cluster_1['Weight']*df_related_cluster_1['pred']\n",
    "# centers = df_1[['NewLat','NewLon']].drop_duplicates()\n",
    "# centers['Weight'] = df_1['Weight']\n",
    "\n",
    "locations = centers[['NewLat', 'NewLon']]\n",
    "weights = centers['Weight']\n",
    "fig = gmaps.figure() \n",
    "heatmap_layer = gmaps.heatmap_layer(locations, weights=weights) \n",
    "fig.add_layer(gmaps.heatmap_layer(locations, weights=weights)) \n",
    "\n",
    "embed_minimal_html('export.html', views=[fig])\n",
    "IPython.display.HTML(filename=\"export.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IdhI_pNPjfkJ"
   },
   "outputs": [],
   "source": [
    "avoid_area = centers.sort_values(by=['Weight'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJ40u3zrjfkK"
   },
   "outputs": [],
   "source": [
    "avoid_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ICC-9HhOjfkL"
   },
   "outputs": [],
   "source": [
    "avoid_list = avoid_area[['NewLat','NewLon']].apply(lambda x: 'bbox:' + str(x['NewLon']-0.00125) + ',' + str(x['NewLat']-0.00125) + \n",
    "                                                   ',' + str(x['NewLon']+0.00125) + ',' + str(x['NewLat']+0.00125), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOsEoXOHjfkL"
   },
   "outputs": [],
   "source": [
    "avoid_param = '|'.join(avoid_list)\n",
    "#'avoid[areas]':'bbox:-122.406046,37.781438,-122.404866,37.782328|bbox:-122.413149,37.780536,-122.410864,37.781469|bbox:-122.406594,37.777967,-122.404946,37.779213',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvae56gHjfkM"
   },
   "outputs": [],
   "source": [
    "avoid_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iue_kWoyjfkM"
   },
   "outputs": [],
   "source": [
    "avoid_rec = []\n",
    "for i in range(len(avoid_list)):\n",
    "  area_label = avoid_list[i][5:].split(',')\n",
    "  avoid_rec.append((float(area_label[3]), float(area_label[0]), float(area_label[1]), float(area_label[2])))\n",
    "  # let temp = new H.map.Rect(new H.geo.Rect(area_label[3], area_label[0],area_label[1],area_label[2]))\n",
    "  # avoid_area.append(temp)\n",
    "\n",
    "avoid_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-zNugigjfkN"
   },
   "source": [
    "### kmeans, k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rtu4Q5_VjfkN"
   },
   "outputs": [],
   "source": [
    "df_related_cluster_1_10 = find_related_cluster(act_start_lat, act_start_lon, act_end_lat, act_end_lon, 'Evening', df_1_10_pred)\n",
    "df_related_cluster_1_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MW2IX4KQjfkN"
   },
   "outputs": [],
   "source": [
    "centers = df_related_cluster_1_10[['NewLat','NewLon']].drop_duplicates()\n",
    "# [6,3,4,1,9,7]\n",
    "centers['Weight'] = df_related_cluster_1_10['Weight']*df_related_cluster_1_10['pred']\n",
    "# centers = df_1[['NewLat','NewLon']].drop_duplicates()\n",
    "# centers['Weight'] = df_1['Weight']\n",
    "\n",
    "locations = centers[['NewLat', 'NewLon']]\n",
    "weights = centers['Weight']\n",
    "fig = gmaps.figure() \n",
    "heatmap_layer = gmaps.heatmap_layer(locations, weights=weights) \n",
    "fig.add_layer(gmaps.heatmap_layer(locations, weights=weights)) \n",
    "\n",
    "embed_minimal_html('export.html', views=[fig])\n",
    "IPython.display.HTML(filename=\"export.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTaFetfwjfkW"
   },
   "outputs": [],
   "source": [
    "avoid_area = centers.sort_values(by=['Weight'], ascending=False).head(10)\n",
    "avoid_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMrVumTTjfkX"
   },
   "outputs": [],
   "source": [
    "avoid_list = avoid_area[['NewLat','NewLon']].apply(lambda x: 'bbox:' + str(x['NewLon']-0.00125) + ',' + str(x['NewLat']-0.00125) + \n",
    "                                                   ',' + str(x['NewLon']+0.00125) + ',' + str(x['NewLat']+0.00125), axis=1).values\n",
    "avoid_param = '|'.join(avoid_list)\n",
    "avoid_rec = []\n",
    "for i in range(len(avoid_list)):\n",
    "  area_label = avoid_list[i][5:].split(',')\n",
    "  avoid_rec.append((float(area_label[3]), float(area_label[0]), float(area_label[1]), float(area_label[2])))\n",
    "\n",
    "avoid_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTE3L_tFjfkY"
   },
   "outputs": [],
   "source": [
    "avoid_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoXTHQyojfkY"
   },
   "source": [
    "### agg, k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpLlDLV4jfkZ"
   },
   "outputs": [],
   "source": [
    "df_related_cluster_2 = find_related_cluster(act_start_lat, act_start_lon, act_end_lat, act_end_lon, 'Evening', df_2_pred)\n",
    "df_related_cluster_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DzDPnL3Ljfka"
   },
   "outputs": [],
   "source": [
    "centers = df_related_cluster_2[['NewLat','NewLon']].drop_duplicates()\n",
    "# [6,3,4,1,9,7]\n",
    "centers['Weight'] = df_related_cluster_2['Weight']*df_related_cluster_2['pred']\n",
    "# centers = df_1[['NewLat','NewLon']].drop_duplicates()\n",
    "# centers['Weight'] = df_1['Weight']\n",
    "\n",
    "locations = centers[['NewLat', 'NewLon']]\n",
    "weights = centers['Weight']\n",
    "fig = gmaps.figure() \n",
    "heatmap_layer = gmaps.heatmap_layer(locations, weights=weights) \n",
    "fig.add_layer(gmaps.heatmap_layer(locations, weights=weights)) \n",
    "\n",
    "embed_minimal_html('export.html', views=[fig])\n",
    "IPython.display.HTML(filename=\"export.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6iozV0O3jfkh"
   },
   "outputs": [],
   "source": [
    "avoid_area = centers.sort_values(by=['Weight'], ascending=False).head(10)\n",
    "avoid_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Z2_I8iKjfkh"
   },
   "outputs": [],
   "source": [
    "avoid_list = avoid_area[['NewLat','NewLon']].apply(lambda x: 'bbox:' + str(x['NewLon']-0.00125) + ',' + str(x['NewLat']-0.00125) + \n",
    "                                                   ',' + str(x['NewLon']+0.00125) + ',' + str(x['NewLat']+0.00125), axis=1).values\n",
    "avoid_param = '|'.join(avoid_list)\n",
    "avoid_rec = []\n",
    "for i in range(len(avoid_list)):\n",
    "  area_label = avoid_list[i][5:].split(',')\n",
    "  avoid_rec.append((float(area_label[3]), float(area_label[0]), float(area_label[1]), float(area_label[2])))\n",
    "\n",
    "avoid_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1Jv_6bEjfki"
   },
   "outputs": [],
   "source": [
    "avoid_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLyN67mWjfkj"
   },
   "source": [
    "### agg, k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rurVjAKRjfkj"
   },
   "outputs": [],
   "source": [
    "df_related_cluster_2_10 = find_related_cluster(act_start_lat, act_start_lon, act_end_lat, act_end_lon, 'Evening', df_2_10_pred)\n",
    "df_related_cluster_2_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riJtOZNKjfkj"
   },
   "outputs": [],
   "source": [
    "centers = df_related_cluster_2_10[['NewLat','NewLon']].drop_duplicates()\n",
    "# [6,3,4,1,9,7]\n",
    "centers['Weight'] = df_related_cluster_2_10['Weight']*df_related_cluster_2_10['pred']\n",
    "# centers = df_1[['NewLat','NewLon']].drop_duplicates()\n",
    "# centers['Weight'] = df_1['Weight']\n",
    "\n",
    "locations = centers[['NewLat', 'NewLon']]\n",
    "weights = centers['Weight']\n",
    "fig = gmaps.figure() \n",
    "heatmap_layer = gmaps.heatmap_layer(locations, weights=weights) \n",
    "fig.add_layer(gmaps.heatmap_layer(locations, weights=weights)) \n",
    "\n",
    "embed_minimal_html('export.html', views=[fig])\n",
    "IPython.display.HTML(filename=\"export.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SIVeDM8ljfkq"
   },
   "outputs": [],
   "source": [
    "avoid_area = centers.sort_values(by=['Weight'], ascending=False).head(10)\n",
    "avoid_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oMFrI6e8jfkr"
   },
   "outputs": [],
   "source": [
    "avoid_list = avoid_area[['NewLat','NewLon']].apply(lambda x: 'bbox:' + str(x['NewLon']-0.00125) + ',' + str(x['NewLat']-0.00125) + \n",
    "                                                   ',' + str(x['NewLon']+0.00125) + ',' + str(x['NewLat']+0.00125), axis=1).values\n",
    "avoid_param = '|'.join(avoid_list)\n",
    "avoid_rec = []\n",
    "for i in range(len(avoid_list)):\n",
    "  area_label = avoid_list[i][5:].split(',')\n",
    "  avoid_rec.append((float(area_label[3]), float(area_label[0]), float(area_label[1]), float(area_label[2])))\n",
    "\n",
    "avoid_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5m_PsJ-jfkr"
   },
   "outputs": [],
   "source": [
    "avoid_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-N_iN4tjfkr"
   },
   "source": [
    "### gmm, k=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVRZyiZHjfkr"
   },
   "outputs": [],
   "source": [
    "df_related_cluster_3 = find_related_cluster(act_start_lat, act_start_lon, act_end_lat, act_end_lon, 'Evening', df_3_pred)\n",
    "df_related_cluster_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUMdJs73jfks"
   },
   "outputs": [],
   "source": [
    "centers = df_related_cluster_3[['NewLat','NewLon']].drop_duplicates()\n",
    "# [6,3,4,1,9,7]\n",
    "centers['Weight'] = df_related_cluster_3['Weight']*df_related_cluster_3['pred']\n",
    "# centers = df_1[['NewLat','NewLon']].drop_duplicates()\n",
    "# centers['Weight'] = df_1['Weight']\n",
    "\n",
    "locations = centers[['NewLat', 'NewLon']]\n",
    "weights = centers['Weight']\n",
    "fig = gmaps.figure() \n",
    "heatmap_layer = gmaps.heatmap_layer(locations, weights=weights) \n",
    "fig.add_layer(gmaps.heatmap_layer(locations, weights=weights)) \n",
    "\n",
    "embed_minimal_html('export.html', views=[fig])\n",
    "IPython.display.HTML(filename=\"export.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9aNNvB8jfky"
   },
   "outputs": [],
   "source": [
    "avoid_area = centers.sort_values(by=['Weight'], ascending=False).head(10)\n",
    "avoid_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMfpvfeJjfkz"
   },
   "outputs": [],
   "source": [
    "avoid_list = avoid_area[['NewLat','NewLon']].apply(lambda x: 'bbox:' + str(x['NewLon']-0.00125) + ',' + str(x['NewLat']-0.00125) + \n",
    "                                                   ',' + str(x['NewLon']+0.00125) + ',' + str(x['NewLat']+0.00125), axis=1).values\n",
    "avoid_param = '|'.join(avoid_list)\n",
    "avoid_rec = []\n",
    "for i in range(len(avoid_list)):\n",
    "  area_label = avoid_list[i][5:].split(',')\n",
    "  avoid_rec.append((float(area_label[3]), float(area_label[0]), float(area_label[1]), float(area_label[2])))\n",
    "\n",
    "avoid_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S6cy2Cedjfkz"
   },
   "outputs": [],
   "source": [
    "avoid_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Lo96Kqjjfkz"
   },
   "source": [
    "### gmm,k =16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-DaQ8kYjfk0"
   },
   "outputs": [],
   "source": [
    "df_related_cluster_3_16 = find_related_cluster(act_start_lat, act_start_lon, act_end_lat, act_end_lon, 'Evening', df_3_16_pred)\n",
    "df_related_cluster_3_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xTac0cirjfk0"
   },
   "outputs": [],
   "source": [
    "centers = df_related_cluster_3_16[['NewLat','NewLon']].drop_duplicates()\n",
    "# [6,3,4,1,9,7]\n",
    "centers['Weight'] = df_related_cluster_3_16['Weight']*df_related_cluster_3_16['pred']\n",
    "# centers = df_1[['NewLat','NewLon']].drop_duplicates()\n",
    "# centers['Weight'] = df_1['Weight']\n",
    "\n",
    "locations = centers[['NewLat', 'NewLon']]\n",
    "weights = centers['Weight']\n",
    "fig = gmaps.figure() \n",
    "heatmap_layer = gmaps.heatmap_layer(locations, weights=weights) \n",
    "fig.add_layer(gmaps.heatmap_layer(locations, weights=weights)) \n",
    "\n",
    "embed_minimal_html('export.html', views=[fig])\n",
    "IPython.display.HTML(filename=\"export.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dydsaY85jfk-"
   },
   "outputs": [],
   "source": [
    "avoid_area = centers.sort_values(by=['Weight'], ascending=False).head(10)\n",
    "avoid_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAavGQAvjfk_"
   },
   "outputs": [],
   "source": [
    "avoid_list = avoid_area[['NewLat','NewLon']].apply(lambda x: 'bbox:' + str(x['NewLon']-0.00125) + ',' + str(x['NewLat']-0.00125) + \n",
    "                                                   ',' + str(x['NewLon']+0.00125) + ',' + str(x['NewLat']+0.00125), axis=1).values\n",
    "avoid_param = '|'.join(avoid_list)\n",
    "avoid_rec = []\n",
    "for i in range(len(avoid_list)):\n",
    "  area_label = avoid_list[i][5:].split(',')\n",
    "  avoid_rec.append((float(area_label[3]), float(area_label[0]), float(area_label[1]), float(area_label[2])))\n",
    "\n",
    "avoid_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rP-JSz-tjfk_"
   },
   "outputs": [],
   "source": [
    "avoid_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3R3I0s-pjflA"
   },
   "source": [
    "### avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0B3VppNGjflB"
   },
   "outputs": [],
   "source": [
    "df_related_cluster_avg = find_related_cluster(act_start_lat, act_start_lon, act_end_lat, act_end_lon, 'Evening', df_avg_pred)\n",
    "df_related_cluster_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fI0Wg6TSjflC"
   },
   "outputs": [],
   "source": [
    "centers = df_related_cluster_avg[['NewLat','NewLon']].drop_duplicates()\n",
    "# [6,3,4,1,9,7]\n",
    "centers['Weight'] = df_related_cluster_avg['Weight']*df_related_cluster_avg['pred']\n",
    "# centers = df_1[['NewLat','NewLon']].drop_duplicates()\n",
    "# centers['Weight'] = df_1['Weight']\n",
    "\n",
    "locations = centers[['NewLat', 'NewLon']]\n",
    "weights = centers['Weight']\n",
    "fig = gmaps.figure() \n",
    "heatmap_layer = gmaps.heatmap_layer(locations, weights=weights) \n",
    "fig.add_layer(gmaps.heatmap_layer(locations, weights=weights)) \n",
    "\n",
    "embed_minimal_html('export.html', views=[fig])\n",
    "IPython.display.HTML(filename=\"export.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENlIeQJajflJ"
   },
   "outputs": [],
   "source": [
    "avoid_area = centers.sort_values(by=['Weight'], ascending=False).head(10)\n",
    "avoid_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vSsCFqVIjflK"
   },
   "outputs": [],
   "source": [
    "avoid_list = avoid_area[['NewLat','NewLon']].apply(lambda x: 'bbox:' + str(x['NewLon']-0.00125) + ',' + str(x['NewLat']-0.00125) + \n",
    "                                                   ',' + str(x['NewLon']+0.00125) + ',' + str(x['NewLat']+0.00125), axis=1).values\n",
    "avoid_param = '|'.join(avoid_list)\n",
    "avoid_rec = []\n",
    "for i in range(len(avoid_list)):\n",
    "  area_label = avoid_list[i][5:].split(',')\n",
    "  avoid_rec.append((float(area_label[3]), float(area_label[0]), float(area_label[1]), float(area_label[2])))\n",
    "\n",
    "avoid_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcNZOzQ7jflK"
   },
   "outputs": [],
   "source": [
    "avoid_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXFz6LjfYbeZ"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AoCHsyHvY2vJ"
   },
   "source": [
    "Based on results from \"Compare w test set\" session, we conclude that:\n",
    "\n",
    "If we want best recall, agg,10 is the best;\n",
    "\n",
    "If we want a balance overall, gmm,4 / gmm,16 is the best;\n",
    "\n",
    "Based on the results from \"Compare by routes\", we conclude that:\n",
    "\n",
    "Either agg, 10 or gmm, 16 provides the best route result.\n",
    "\n",
    "But agg is more stable than gmm based on their algorithm. So we choose agg, 10 as our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGdUPmaEYqYZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "trN3wfns4AJS",
    "S1VMmqNCcHth",
    "82u8ZM3-K52C",
    "rfUzFKH1q9oL",
    "f1JSoZQ8xfmj",
    "wEAB5SPDxooq",
    "0L2QL7iWEEXv",
    "8iADYe2byB89",
    "CgfFvzhYyEQq",
    "AvilljsXIllc",
    "G3Vd6gjwyLZQ",
    "_tjc5owRiRV4",
    "4qpVYEY3Fsz_",
    "lkhZU8H2ANfQ",
    "g9mfsJ-VFvsR",
    "dAxqYMmdAUqq",
    "CoRIy1Z2J5Zy",
    "Ue59h6muAf1N",
    "5yd7HTSPCFGP",
    "Bgptn70sZ9HE",
    "89xv6g-3kMzA",
    "d1HIi57nt7KJ",
    "ihgJyc8uurSL",
    "kP9mF4rMw9sB",
    "LYrYNfhixM9b",
    "vavwL0f1zWFa",
    "P-6C_An6YFv7",
    "9EpaDziOFqCe",
    "5D5d4FSKXXOR",
    "7HneW6MEaM_I",
    "kEtnTqgpbuc8",
    "dG7baFQ_dE-3",
    "iaZyjycLeVnP",
    "SE4AuN-Sfsei",
    "Jl2M1u5Cjfjr",
    "1-zNugigjfkN",
    "BoXTHQyojfkY",
    "qLyN67mWjfkj",
    "U-N_iN4tjfkr",
    "4Lo96Kqjjfkz",
    "3R3I0s-pjflA",
    "lXFz6LjfYbeZ"
   ],
   "name": "Modeling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
